---
title: "Qualitative Activity Recognition"
author: "Monica Brisnehan, October 2016"
output: html_document
---

#Synopsis

Human activity recognition is becoming an ever-important area of research, resulting in the collection of an increasing amount of movement data. This analysis uses data generated by the GroupwareLES, and more information can be found [here]( http://groupware.les.inf.puc-rio.br/har).

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

This analysis creates two machine learning models, resulting in a random forest model with estimated 99.2% accuracy in predicting if an exercise movement was performed correctly based on movement readings.

## Data Source

The data used to create a movement model is found [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv), and the data used to test the model is found [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

## Loading and Tidying Data

Prior to analysis of the data, the appropriate R libraries must be loaded into the current environment

```{r load_libraries, message=FALSE}
## Load required libraries using new package Install_Load
if(!require(install.load) ) {
  install.packages("install.load")
}
library(install.load)
install_load("dplyr", "dtplyr", "tidyr", "stringr", "ggplot2", "RColorBrewer", "knitr","caret","rattle","corrplot", "rpart", "rpart.plot", "parallel","doParallel")
```

```{r echo=FALSE, message=FALSE}
knit_hooks$set(inline = function(x) {
  prettyNum(round(x,2), big.mark=",")
})
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First, we need to download the data and read each data set into a data frame.
```{r load_data, cache=TRUE}
if (!file.exists("data")) {
    dir.create("data")
    }
trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"    
download.file(trainURL, destfile = "./data/pml-training.csv")
download.file(testURL, destfile = "./data/pml-testing.csv")
trainData <- read.csv("./data/pml-training.csv", na.strings=c("","NA","NaN"))
testData <- read.csv("./data/pml-testing.csv", na.strings=c("","NA","NaN"))
```
The raw training data set has `r nrow(trainData)` observations of `r ncol(trainData)` variables. However, as shown below, some columns have NA/null data. We'll remove the columns that have more than 90% "NA" or missing data. Also, we'll remove the first seven columns (non-measurement or identifier variables) to result in a clean dataset for modeling. 

```{r clear_NA}
trainData <- trainData[, colSums(is.na(trainData)) < 0.9 * nrow(trainData)] 
testData <- testData[, colSums(is.na(testData)) < 0.9 * nrow(testData)]
trainData <- trainData[, -(1:7)]
dim(trainData)
```

Applying a Near Zero Variance analysis indicates that all remaining factors are useful.

``` {r near_zero}
nsv <- nearZeroVar(trainData,saveMetrics=TRUE)
```

However, we can remove highly correlated variables by eliminating variables of 0.85 correlation or higher.

``` {r corr_matrix}
trainCor <- cor(trainData[,-53])
hiCorr <- findCorrelation(trainCor, cutoff=0.85, verbose=FALSE)
trainData <- subset(trainData, select=-hiCorr)
```

We are left with a training set of `r nrow(trainData)` observations of `r ncol(trainData)` variables/features. For cross-validation purposes, let's split the training data into a training subset and a validation subset. For this analysis, we'll use a 70/30 split for training/validation (as we don't need a test set).

```{r split_training}
set.seed(1129)
inTrain <- createDataPartition(y=trainData$classe, p=0.7, list=FALSE)
training <- trainData[inTrain,]
testing <- trainData[-inTrain,]
dim(training);dim(testing)
```

##Model using various methods

### Classification Tree Model

One of the simpler models for categorizing outcomes is the classification tree. Variables are separated into groups, and decision nodes create an algorithmic path to categorizing outcomes. We'll use the Rpart functionality of R and caret to model the dataset.
``` {r rpart_model, cache=TRUE}
modRpart <- train(classe ~ .,method="rpart",data=training)
fancyRpartPlot(modRpart$finalModel)

predRpart <- predict(modRpart, testing)
accRp <- confusionMatrix(predRpart, testing$classe)$overall[1]
accRp
```

The relatively low accuracy of `r accRp*100`% leads the analysis to a different algorithm, Random Forest.

### Random Forest Model

In an effort to increase accuracy, we can perform a random forest analysis, which creates a large number of decision trees of lower accuracy and averages their outcome, resulting in higher accuracy.
``` {r rf_model, cache=TRUE}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
ctrl<- trainControl(method = "cv", number = 4, allowParallel = TRUE)
modRF <- train(classe ~ ., method="rf", trControl=ctrl, data=training, importance=TRUE, proximity=FALSE)
stopCluster(cluster)
registerDoSEQ()
modRF$finalModel
predRF <- predict(modRF, testing)
accRf <- confusionMatrix(predRF, testing$classe)$overall[1]
accRf
```
The accuracy of `r accRf*100`% as performed on the validation data is much higher than the RPart model and gives confidence that this model will perform better on the test data. The out-of-sample error is estimated at `r (1-accRf)*100`%.

### Boosting Model

One additional method for classification is gradient boosting, which combines weak predictors to form stronger predictors. We can use caret's gbm method to model our data.
``` {r gbm_model, cache=TRUE}
modGbm <- train(classe ~ ., method="gbm",data=training, verbose=FALSE)
predGbm <- predict(modGbm,testing)
accGbm <- confusionMatrix(predGbm,testing$classe)$overall[1]
accGbm
```
The accuracy of `r accGbm*100`% as performed on the validation data is much higher than the RPart model but lower than that of the random forest model. The out-of-sample error is estimated at `r (1-accGbm)*100`%.

##Application of Random Forest Model to Test Data

Using the random forest model, which has the highest estimated accuracy, we can estimate the following vector of classe predictions:
```{r rf_test, message=FALSE}
testRF <- predict(modRF, testData)
testRF
```

